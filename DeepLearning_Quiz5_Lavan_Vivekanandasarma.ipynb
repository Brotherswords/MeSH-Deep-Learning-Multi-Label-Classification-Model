{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOXMWS4thwPixkOAuzbnDSu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Brotherswords/MeSH-Deep-Learning-Multi-Label-Classification-Model/blob/main/DeepLearning_Quiz5_Lavan_Vivekanandasarma.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Quiz 5 (Take Home) Assignment"
      ],
      "metadata": {
        "id": "EguPhRK4w6k1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing Neccesary Libraries\n"
      ],
      "metadata": {
        "id": "apjQ4J1exBNz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0WfuIEEnQoT3",
        "outputId": "80a87310-c983-4271-a5eb-5c142b908309"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.14.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes==0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.34.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.59.2)\n",
            "Requirement already satisfied: tensorboard<2.15,>=2.14 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.14.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.14.0)\n",
            "Requirement already satisfied: keras<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.14.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.41.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (3.5.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (3.2.2)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.14.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow\n",
        "!pip install keras\n",
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "kYIvTUhzw51F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import zipfile\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DY623wOdw0SW",
        "outputId": "7180b885-47cd-4553-bad2-80cb2821cc56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Check for the file I need\n",
        "directory_path = '/content/drive/My Drive/Projects Things Useful/University/CU Denver 2023-2024 Sem 1/Deep Learning/Quiz_5_Data'\n",
        "pickle_path = directory_path + \"/Pickle_Files\"\n",
        "# Check if the directory exists\n",
        "if os.path.exists(directory_path):\n",
        "    # List all files and directories in the specified path\n",
        "    files = os.listdir(directory_path)\n",
        "    print(\"Files and directories in '\", directory_path, \"' :\")\n",
        "    for i in files:\n",
        "      print(i)\n",
        "else:\n",
        "    print(\"The directory does not exist\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZNlh99RxHJx",
        "outputId": "14f7d933-61e5-4cdd-ec96-9ab7d509c042"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files and directories in ' /content/drive/My Drive/Projects Things Useful/University/CU Denver 2023-2024 Sem 1/Deep Learning/Quiz_5_Data ' :\n",
            "training-set-100000.json\n",
            "test-set-20000-rev2.json\n",
            "judge-set-10000-unannotated.json\n",
            "Pickle_Files\n",
            "BioWordVec_PubMed_MIMICIII_d200.vec.bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "file_name= \"/training-set-100000.json\"\n",
        "training_path = f'{directory_path}/{file_name}'\n",
        "data = []\n",
        "with open(training_path, 'r') as file:\n",
        "    data = json.load(file)\n",
        "    if data:\n",
        "      print(\"Training Data Loaded\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0720XWhCx-_U",
        "outputId": "70cc65bf-0ce7-46c9-f535-2df348b2bad8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Data Loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1. Data Pre-Processing"
      ],
      "metadata": {
        "id": "-EP2bpHe4Ha7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "# Download stopwords and wordnet data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Initialize stemmer and lemmatizer\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Load English stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def clean_text(text):\n",
        "    # Remove HTML tags using regex\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "\n",
        "    # Remove special characters and digits\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "    # Lowercase all texts\n",
        "    text = text.lower()\n",
        "\n",
        "    # Tokenize text\n",
        "    words = text.split()\n",
        "\n",
        "    # Remove stopwords and apply stemming or lemmatization\n",
        "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
        "    # Uncomment the next line to use stemming instead of lemmatization\n",
        "    # words = [stemmer.stem(word) for word in words if word not in stop_words]\n",
        "\n",
        "    # Join the words back into a single string\n",
        "    text = ' '.join(words)\n",
        "\n",
        "    return text\n",
        "\n",
        "# cleaned_texts = [clean_text(text) for text in texts]\n",
        "\n",
        "# # Now `cleaned_texts` contains the preprocessed article texts\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZimqFVBH67-q",
        "outputId": "084087c5-bc85-4c32-d80c-5a2506dbddef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming `data` is already loaded with the articles data in the above format\n",
        "# Combine title and abstract for each article and gather labels\n",
        "texts = [clean_text(d['title'] + ' ' + d['abstractText']) for d in data['articles']]\n",
        "labels = [d['meshMajor'] for d in data['articles']]\n",
        "\n",
        "# Initialize the tokenizer with a specific vocabulary size, and a filter for punctuation\n",
        "vocabulary_size = 15000  # for example, you might choose the top 20,000 words\n",
        "tokenizer = Tokenizer(num_words=vocabulary_size, lower=True, oov_token='UNK', filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
        "\n",
        "tokenizer.fit_on_texts(texts)\n",
        "\n",
        "# Convert texts to sequences of integer indices\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "# Determine a suitable maximum sequence length based on the data distribution\n",
        "# For instance, you can take the 90th percentile as the max length\n",
        "max_seq_length = int(np.percentile([len(seq) for seq in sequences], 75))\n",
        "\n",
        "# Pad the sequences so that they all have the same length\n",
        "data_padded = pad_sequences(sequences, maxlen=max_seq_length, padding='post')\n",
        "\n",
        "# Initialize MultiLabelBinarizer to convert the labels to a binary matrix\n",
        "mlb = MultiLabelBinarizer()\n",
        "label_data = mlb.fit_transform(labels)\n",
        "\n",
        "# This absolutely devours material.\n",
        "# # Save the tokenizer, MultiLabelBinarizer, max_seq_length for later use\n",
        "# with open(pickle_path + '/preprocessing_objects.pickle', 'wb') as handle:\n",
        "#     pickle.dump({'tokenizer': tokenizer, 'mlb': mlb, 'max_seq_length': max_seq_length,\n",
        "#                  'data_padded': data_padded, 'label_data': label_data}, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "# Now `data_padded` contains the padded sequences and `label_data` contains the multi-hot encoded labels\n",
        "# `word_index` contains the word index, and `max_seq_length` is the length up to which sequences will be padded\n",
        "print(\"Preprocessing and saving completed.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dCFn5m8tzfKr",
        "outputId": "df54f4db-0330-46c9-a1cc-efd3770fbba5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing and saving completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Load all preprocessing objects at once\n",
        "# with open(pickle_path + '/preprocessing_objects.pickle', 'rb') as handle:\n",
        "#     preprocessing_objects = pickle.load(handle)\n",
        "\n",
        "# tokenizer = preprocessing_objects['tokenizer']\n",
        "# mlb = preprocessing_objects['mlb']\n",
        "# max_seq_length = preprocessing_objects['max_seq_length']\n",
        "# # data_padded = preprocessing_objects['data_padded']\n",
        "# label_data = preprocessing_objects['label_data']\n",
        "# print(\"Loaded\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TQIVhNyS0fb-",
        "outputId": "bda0df7d-1eea-44aa-9eb7-d25783678dee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n = 10\n",
        "for i, (word, index) in enumerate(tokenizer.word_index.items()):\n",
        "    print(f\"{i + 1}. {word}: {index}\")\n",
        "    if i >= n - 1:  # since index starts at 0, we use n - 1\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EFVdE5Z_0sj_",
        "outputId": "47ce1b69-2fe8-4ee1-9046-312ea5608040"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. UNK: 1\n",
            "2. patient: 2\n",
            "3. cell: 3\n",
            "4. study: 4\n",
            "5. result: 5\n",
            "6. protein: 6\n",
            "7. p: 7\n",
            "8. group: 8\n",
            "9. effect: 9\n",
            "10. level: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2. Creating the Embedded Layer 🐪"
      ],
      "metadata": {
        "id": "NcbaalUyjYfo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RUN THIS ONCE, AFTERWARDS JUST USE PICKLE FILE VERSION\n",
        "(IT TAKES FOREVER TO LOAD)"
      ],
      "metadata": {
        "id": "NaRWhCcZ9ItM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import KeyedVectors\n",
        "\n",
        "embedding_path = directory_path + \"/BioWordVec_PubMed_MIMICIII_d200.vec.bin\"\n",
        "embeddings = KeyedVectors.load_word2vec_format(embedding_path, binary=True)\n",
        "\n",
        "\n",
        "\n",
        "# Get the word_index from your tokenizer\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "# Initialize the embedding matrix\n",
        "embedding_matrix = np.zeros((len(word_index) + 1, 200))  # Add 1 for padding token\n",
        "\n",
        "# Populate the embedding matrix\n",
        "for word, i in word_index.items():\n",
        "    # Check if the word is in the model\n",
        "    if word in embeddings.key_to_index:\n",
        "        # Get the embedding vector for the word\n",
        "        embedding_vector = embeddings[word]\n",
        "        # If an embedding was found, add it to the matrix\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "\n",
        "with open(pickle_path + \"/embedding_matrix_15000\", 'wb') as handle:\n",
        "    pickle.dump(embedding_matrix, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "print(\"Embedding matrix saved.\")\n"
      ],
      "metadata": {
        "id": "UEl8T0_977FG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f5603b3-5f82-4c68-b027-e10720b5304d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding matrix saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the embedding layer with pre-trained weights\n",
        "\n",
        "loaded_embedding_matrix = []\n",
        "with open(pickle_path + \"/embedding_matrix_15000\", 'rb') as handle:\n",
        "    loaded_embedding_matrix = pickle.load(handle)\n",
        "\n",
        "embedding_layer = Embedding(input_dim=len(word_index) + 1,\n",
        "                            output_dim=200,\n",
        "                            weights=[loaded_embedding_matrix],\n",
        "                            input_length=max_seq_length,  # As determined earlier\n",
        "                            trainable=False)"
      ],
      "metadata": {
        "id": "IGFwQ9ZOqsQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3. Create LSTM layers"
      ],
      "metadata": {
        "id": "kpYZx6fp8I4g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of unique labels in your multi-label classification task\n",
        "num_labels = len(mlb.classes_)  # Replace with actual number of labels\n",
        "\n",
        "model = Sequential()\n",
        "# Add the pre-loaded embedding layer\n",
        "model.add(embedding_layer)  # embedding_layer is the one you created using loaded_embedding_matrix\n",
        "# Add an LSTM layer\n",
        "model.add(Bidirectional(LSTM(units=128, return_sequences=True)))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Bidirectional(LSTM(units=64)))\n",
        "# model.add(Dropout(0.2))\n",
        "# Add a dense layer\n",
        "model.add(Dense(units=128, activation='relu'))\n",
        "#model.add(Dropout(0.2))\n",
        "# Output layer with a sigmoid activation for multi-label classification\n",
        "model.add(Dense(units=num_labels, activation='sigmoid'))\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "DkH5z7NCu01o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e8524a2-4ad5-436b-81b9-d29432ed8430"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 155, 200)          54306400  \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 155, 256)         336896    \n",
            " l)                                                              \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 155, 256)          0         \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirectio  (None, 128)              164352    \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               16512     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 22373)             2886117   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 57,710,277\n",
            "Trainable params: 3,403,877\n",
            "Non-trainable params: 54,306,400\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4. Train the model and pray that Google Colab Pro is Enough!"
      ],
      "metadata": {
        "id": "yCTYO_I6CTxe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the training data\n",
        "X_train = data_padded  # Your padded sequence data\n",
        "Y_train = label_data   # Your multi-hot encoded MeSH terms"
      ],
      "metadata": {
        "id": "5pEIdRfNRcBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.metrics import Recall\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[Recall()])\n",
        "\n",
        "\n",
        "# Set training parameters\n",
        "batch_size = 32\n",
        "epochs = 25\n",
        "validation_split = 0.1  # Percentage of data to use as validation\n",
        "\n",
        "# Define an EarlyStopping callback to prevent overfitting\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, Y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    validation_split=validation_split,\n",
        "                    callbacks=[early_stopping],\n",
        "                    verbose=1)\n",
        "\n",
        "print(\"Training completed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TeLL-Zfs8PQv",
        "outputId": "811e0920-cf91-4417-dae9-79ed7853b76f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "2813/2813 [==============================] - 553s 194ms/step - loss: 0.0068 - recall: 0.0480 - val_loss: 0.0037 - val_recall: 0.0475\n",
            "Epoch 2/25\n",
            "2813/2813 [==============================] - 548s 195ms/step - loss: 0.0036 - recall: 0.0492 - val_loss: 0.0036 - val_recall: 0.0576\n",
            "Epoch 3/25\n",
            "2813/2813 [==============================] - 547s 194ms/step - loss: 0.0034 - recall: 0.0980 - val_loss: 0.0032 - val_recall: 0.1014\n",
            "Epoch 4/25\n",
            "2813/2813 [==============================] - 549s 195ms/step - loss: 0.0030 - recall: 0.1249 - val_loss: 0.0029 - val_recall: 0.1302\n",
            "Epoch 5/25\n",
            "2813/2813 [==============================] - 547s 195ms/step - loss: 0.0028 - recall: 0.1349 - val_loss: 0.0027 - val_recall: 0.1434\n",
            "Epoch 6/25\n",
            "2813/2813 [==============================] - 548s 195ms/step - loss: 0.0026 - recall: 0.1453 - val_loss: 0.0026 - val_recall: 0.1520\n",
            "Epoch 7/25\n",
            "2813/2813 [==============================] - 548s 195ms/step - loss: 0.0025 - recall: 0.1521 - val_loss: 0.0025 - val_recall: 0.1467\n",
            "Epoch 8/25\n",
            "2813/2813 [==============================] - 548s 195ms/step - loss: 0.0024 - recall: 0.1591 - val_loss: 0.0024 - val_recall: 0.1623\n",
            "Epoch 9/25\n",
            "2813/2813 [==============================] - 549s 195ms/step - loss: 0.0023 - recall: 0.1675 - val_loss: 0.0024 - val_recall: 0.1737\n",
            "Epoch 10/25\n",
            "2813/2813 [==============================] - 549s 195ms/step - loss: 0.0023 - recall: 0.1757 - val_loss: 0.0023 - val_recall: 0.1848\n",
            "Epoch 11/25\n",
            "2813/2813 [==============================] - 548s 195ms/step - loss: 0.0022 - recall: 0.1846 - val_loss: 0.0023 - val_recall: 0.1933\n",
            "Epoch 12/25\n",
            "2813/2813 [==============================] - 549s 195ms/step - loss: 0.0022 - recall: 0.1930 - val_loss: 0.0023 - val_recall: 0.1947\n",
            "Epoch 13/25\n",
            "2813/2813 [==============================] - 549s 195ms/step - loss: 0.0021 - recall: 0.1998 - val_loss: 0.0023 - val_recall: 0.1941\n",
            "Epoch 14/25\n",
            "2813/2813 [==============================] - 547s 194ms/step - loss: 0.0021 - recall: 0.2063 - val_loss: 0.0022 - val_recall: 0.1997\n",
            "Epoch 15/25\n",
            "2813/2813 [==============================] - 548s 195ms/step - loss: 0.0020 - recall: 0.2125 - val_loss: 0.0022 - val_recall: 0.2036\n",
            "Epoch 16/25\n",
            "2813/2813 [==============================] - 547s 194ms/step - loss: 0.0020 - recall: 0.2182 - val_loss: 0.0022 - val_recall: 0.2074\n",
            "Epoch 17/25\n",
            "2813/2813 [==============================] - 547s 194ms/step - loss: 0.0020 - recall: 0.2238 - val_loss: 0.0022 - val_recall: 0.2151\n",
            "Epoch 18/25\n",
            "2813/2813 [==============================] - 548s 195ms/step - loss: 0.0019 - recall: 0.2285 - val_loss: 0.0022 - val_recall: 0.2196\n",
            "Epoch 19/25\n",
            "2813/2813 [==============================] - 547s 194ms/step - loss: 0.0019 - recall: 0.2334 - val_loss: 0.0022 - val_recall: 0.2152\n",
            "Epoch 20/25\n",
            "2813/2813 [==============================] - 547s 194ms/step - loss: 0.0019 - recall: 0.2383 - val_loss: 0.0022 - val_recall: 0.2271\n",
            "Epoch 21/25\n",
            "2813/2813 [==============================] - 547s 195ms/step - loss: 0.0019 - recall: 0.2425 - val_loss: 0.0022 - val_recall: 0.2253\n",
            "Epoch 22/25\n",
            "2813/2813 [==============================] - 547s 194ms/step - loss: 0.0018 - recall: 0.2469 - val_loss: 0.0022 - val_recall: 0.2259\n",
            "Epoch 23/25\n",
            "2813/2813 [==============================] - 546s 194ms/step - loss: 0.0018 - recall: 0.2510 - val_loss: 0.0022 - val_recall: 0.2288\n",
            "Epoch 23: early stopping\n",
            "Training completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_save_path = pickle_path + '/saved_model_25_Epochs_Recall.h5'  # Replace with your desired path\n"
      ],
      "metadata": {
        "id": "buit2vLFRNuR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "# V1 of model\n",
        "# model_save_path = pickle_path + '/saved_model.h5'  # Replace with your desired path\n",
        "model.save(model_save_path)\n",
        "print(f\"Model saved at {model_save_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YllZAxWLCntW",
        "outputId": "15da3763-89c8-45dc-8c10-f6d37f704e90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved at /content/drive/My Drive/Projects Things Useful/University/CU Denver 2023-2024 Sem 1/Deep Learning/Quiz_5_Data/Pickle_Files/saved_model_25_Epochs_Recall.h5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5. Call the saved model and run it. Give it a test run to see we're getting real results."
      ],
      "metadata": {
        "id": "8ESbBHjojNeq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load the model\n",
        "loaded_model = load_model(model_save_path)\n",
        "print(\"Model loaded successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X46pb-AbOPmz",
        "outputId": "268acdf9-dacd-4843-9193-e615e548e5ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "first_sample = X_train[1:2]  # Select the first sample\n",
        "predicted_result = loaded_model.predict(first_sample)  # Predict using the loaded model\n",
        "\n",
        "# Print the predicted result\n",
        "print(\"Predicted result:\", predicted_result)\n",
        "\n",
        "# Print the actual result\n",
        "actual_result = Y_train[1]\n",
        "print(\"Actual result:\", actual_result)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lak5hJYuheJz",
        "outputId": "9ad2233b-62a5-4f4b-ef33-a011fa6e0783"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 1s/step\n",
            "Predicted result: [[6.98327653e-07 5.76985667e-06 2.49171399e-06 ... 1.08561708e-05\n",
            "  1.24572925e-05 1.07505741e-06]]\n",
            "Actual result: [0 0 0 ... 0 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use inverse_transform to convert binary vectors back to labels\n",
        "import numpy as np\n",
        "\n",
        "# Convert predicted result to binary and then to labels\n",
        "predicted_labels_conformed = mlb.inverse_transform(np.round(predicted_result))\n",
        "\n",
        "# Ensure actual_result is a 2D array for inverse_transform\n",
        "actual_labels_conformed = mlb.inverse_transform(np.array([actual_result]))\n",
        "\n",
        "print(\"Predicted MeSH terms:\", predicted_labels_conformed[0])\n",
        "print(\"Actual MeSH terms:\", actual_labels_conformed[0])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2zG-BuEohxnw",
        "outputId": "5ff1a9b8-a419-4371-af50-29ea02be8cda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted MeSH terms: ('Adult', 'Aged', 'Carcinoma, Squamous Cell', 'Case-Control Studies', 'Female', 'Genetic Predisposition to Disease', 'Genotype', 'Humans', 'Male', 'Middle Aged', 'Polymorphism, Genetic', 'Uterine Cervical Neoplasms')\n",
            "Actual MeSH terms: ('Adult', 'Aged', 'Aged, 80 and over', 'Carcinoma, Squamous Cell', 'Case-Control Studies', 'Female', 'Gene Frequency', 'Genetic Predisposition to Disease', 'Genotype', 'Germany', 'Greece', 'Humans', 'Interleukin-8', 'Male', 'Middle Aged', 'Mouth Neoplasms', 'Polymorphism, Genetic', 'Reverse Transcriptase Polymerase Chain Reaction', 'Risk')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4. Seeing how well we did on the test set 🙏"
      ],
      "metadata": {
        "id": "ad_sRHa-Ofky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the test set\n",
        "import json\n",
        "file_name= \"test-set-20000-rev2.json\"\n",
        "training_path = f'{directory_path}/{file_name}'\n",
        "data = []\n",
        "with open(training_path, 'r') as file:\n",
        "    data = json.load(file)\n",
        "    if data:\n",
        "      print(\"Test Data Loaded\")\n",
        "\n",
        "texts = [clean_text(d['title'] + ' ' + d['abstractText']) for d in data['documents']]\n",
        "labels = [d['meshMajor'] for d in data['documents']]\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "# Pad the sequences so that they all have the same length\n",
        "X_Test = data_padded_test = pad_sequences(sequences, maxlen=max_seq_length, padding='post')\n",
        "Y_Test = label_data_test = mlb.transform(labels)"
      ],
      "metadata": {
        "id": "Dti4cX6xiRib",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0533765-ba2e-464c-ee9f-f692d0c66b14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Data Loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import numpy as np\n",
        "\n",
        "# Make predictions on the test set\n",
        "test_predictions = loaded_model.predict(X_Test)\n",
        "binary_predictions = np.round(test_predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t2WeuvTSO9Fg",
        "outputId": "04c2d983-c6f9-4bac-d786-7cc3d814fe25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "625/625 [==============================] - 45s 70ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(Y_Test[0]))\n",
        "print(len(X_Test[0]))\n",
        "\n",
        "print(Y_Test[0])\n",
        "print(X_Test[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7yL9r_8nkbHj",
        "outputId": "832e9e1a-04fb-4ba0-e40a-087f738042ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22373\n",
            "155\n",
            "[0 0 0 ... 0 0 0]\n",
            "[2058 3561 3506 1692 3315   16    1 1301   11 1271  111 2058 3561 3506\n",
            " 1692 3315  155 1188  416  983   16    1 1301  103 2863  607   42 1301\n",
            "   44   62   11 3506 1247  121 2263   61    1  212 2058 3561 1692 3315\n",
            "  416   81 1301   33  761  416 2263  212 1353  643 2263  106   86  985\n",
            "  212   62 1188   81 1301  510 1365  111   20    5 1423 1379   10 1301\n",
            "   20  766 1081  943 1301    6   62  416 2263    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the metrics\n",
        "accuracy = accuracy_score(Y_Test, binary_predictions)\n",
        "micro_precision = precision_score(Y_Test, binary_predictions, average='micro')\n",
        "micro_recall = recall_score(Y_Test, binary_predictions, average='micro')\n",
        "micro_f1 = f1_score(Y_Test, binary_predictions, average='micro')"
      ],
      "metadata": {
        "id": "CAz108VHoKx2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Accuracy on Test Data:\", accuracy)\n",
        "print(\"Micro Precision on Test Data:\", micro_precision)\n",
        "print(\"Micro Recall on Test Data:\", micro_recall)\n",
        "print(\"Micro F1 Score on Test Data:\", micro_f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QWIWKBXsgCxx",
        "outputId": "141f31c5-5713-4343-e3c2-062ad1b620dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on Test Data: 0.0\n",
            "Micro Precision on Test Data: 0.6947941675938397\n",
            "Micro Recall on Test Data: 0.2274112492381292\n",
            "Micro F1 Score on Test Data: 0.3426655422577515\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Report"
      ],
      "metadata": {
        "id": "o0s0vNqtqCda"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After evaluating my model on the test data for the multi-label classification task, the results suggest that it's moderately effective, but there's significant room for improvement. Notably, the model's accuracy is surprisingly low at 0.0, indicating it's not predicting all the labels for any single sample correctly. This is a known challenge in multi-label classification, where accuracy demands a perfect match of all labels.\n",
        "\n",
        "The micro precision, at 69.48%, shows that a majority of the predicted labels are correct. However, the micro recall is only 22.74%, implying that the model is missing many relevant labels. This is further reflected in the moderate micro F1 score of 34.27%, which balances precision and recall.\n",
        "\n",
        "The discrepancy between precision and recall suggests a conservative model that makes fewer errors in its predictions but overlooks a significant number of correct labels. This could stem from various factors, such as the model's complexity, potential class imbalance in the dataset, or the need for more extensive training. To enhance the model's effectiveness, fine-tuning, and a more in-depth analysis are crucial next steps.\n",
        "\n"
      ],
      "metadata": {
        "id": "be6_G6M4qE6u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 6. Making Predictions for Judge and Writing to JSON"
      ],
      "metadata": {
        "id": "_4qP0uFPqeLW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "{\"documents\": [\n",
        "{\"pmid\": 22511223, \"labels\":[\"label1\",\"label2\",...,\"labelN\"]},\n",
        "{\"pmid\":22511224, \"labels\":[\"label1\", \"label2\",...,\"labelM\"]},\n",
        "...\n",
        "...\n",
        ".\n",
        "{\"pmid\":22511225,\"labels\":[\"label1\", \"label2\",...,\"labelK\"] }\n",
        "]}"
      ],
      "metadata": {
        "id": "ifCLCGgjqd8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the test set\n",
        "import json\n",
        "file_name= \"judge-set-10000-unannotated.json\"\n",
        "training_path = f'{directory_path}/{file_name}'\n",
        "data = []\n",
        "with open(training_path, 'r') as file:\n",
        "    data = json.load(file)\n",
        "    if data:\n",
        "      print(\"Judge Data Loaded\")\n",
        "\n",
        "texts = [clean_text(d['title'] + ' ' + d['abstractText']) for d in data['documents']]\n",
        "labels = [d['pmid'] for d in data['documents']]\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "# Pad the sequences so that they all have the same length\n",
        "X_Judge = data_padded_test = pad_sequences(sequences, maxlen=max_seq_length, padding='post')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hGZ-YORasKkh",
        "outputId": "454f7096-8e1c-4a88-c19d-a2941121236c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Judge Data Loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import numpy as np\n",
        "\n",
        "# Make predictions on the judge set\n",
        "judge_predictions = loaded_model.predict(X_Judge)\n",
        "binary_predictions = np.round(judge_predictions)\n",
        "predicted_labels_conformed = mlb.inverse_transform((binary_predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWphUweIslHJ",
        "outputId": "9ecf75bd-5ccb-42ac-cad5-9bfc604a68f8"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 22s 70ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Prepare data for JSON export\n",
        "json_output = {\"documents\": []}\n",
        "for pmid, labels in zip(labels, predicted_labels_conformed):\n",
        "    json_output[\"documents\"].append({\"pmid\": pmid, \"labels\": list(labels)})\n",
        "\n",
        "# Write the data to a JSON file\n",
        "output_path = f'{directory_path}/predicted_labels.json'\n",
        "with open(output_path, 'w') as file:\n",
        "    json.dump(json_output, file, indent=4)\n",
        "\n",
        "print(f\"Predictions saved to {output_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YjJBXOI7s9TI",
        "outputId": "4b0f44a0-81fb-46d2-85c7-a2468da1245d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions saved to /content/drive/My Drive/Projects Things Useful/University/CU Denver 2023-2024 Sem 1/Deep Learning/Quiz_5_Data/predicted_labels.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jWe__6xItinM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}