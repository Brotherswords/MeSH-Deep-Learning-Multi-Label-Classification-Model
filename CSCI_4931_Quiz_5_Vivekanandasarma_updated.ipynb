{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Brotherswords/MeSH-Deep-Learning-Multi-Label-Classification-Model/blob/main/CSCI_4931_Quiz_5_Vivekanandasarma_updated.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EguPhRK4w6k1"
      },
      "source": [
        "#Quiz 5 (Take Home) Assignment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0OcsvptCsYm"
      },
      "source": [
        "# Overview\n",
        "\n",
        "This Python Notebook illustrates my efforts in creating and evaluating models for the task of assigning MeSH terms for medical articles from PubMed given an abstract. I explore two approaches.\n",
        "\n",
        "1. Using an Embedding Matrix for an Embedding Layer, a unique/specialized word2vec aimed at medical information and LSTMs.\n",
        "\n",
        "2. Using BERT, not as specialized for medical tasks but trained on much more data giving it the ability to find deeper nuances than my simple network.\n",
        "\n",
        "For both, I was severely hindered by my ability to run models with enough compute. Something that (as I note later in my report) likely was a significant factor in the results. Efforts were made to run for longer epochs and more complex models - however computational limits were unforgiving."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apjQ4J1exBNz"
      },
      "source": [
        "## Installing Neccesary Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0WfuIEEnQoT3",
        "outputId": "721df40f-f0fd-47b7-b60b-c957a32b5775"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.12.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.59.2)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.3.25)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.34.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.41.3)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow) (1.11.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (3.5.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.13,>=2.12->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.13,>=2.12->tensorflow) (3.2.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.12.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.59.2)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.3.25)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.34.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.41.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow) (1.11.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (3.5.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (3.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.13,>=2.12->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.13,>=2.12->tensorflow) (3.2.2)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.12.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow\n",
        "!pip install transformers\n",
        "!pip install keras\n",
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYIvTUhzw51F"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DY623wOdw0SW"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "from transformers import TFBertModel, BertTokenizer\n",
        "import os\n",
        "import zipfile\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional, Input\n",
        "from tensorflow.keras.models import Model\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from tensorflow.keras.metrics import Precision, Recall\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.models import load_model\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import numpy as np\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZNlh99RxHJx",
        "outputId": "8f81b09c-e36a-4786-bf43-146def582242"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Files and directories in ' /content/drive/My Drive/Projects Things Useful/University/CU Denver 2023-2024 Sem 1/Deep Learning/Quiz_5_Data ' :\n",
            "training-set-100000.json\n",
            "test-set-20000-rev2.json\n",
            "judge-set-10000-unannotated.json\n",
            "Pickle_Files\n",
            "BioWordVec_PubMed_MIMICIII_d200.vec.bin\n",
            "predicted_labels.json\n",
            "bert-models\n",
            "lstm_inter\n"
          ]
        }
      ],
      "source": [
        "#Check for the file I need\n",
        "drive.mount('/content/drive')\n",
        "directory_path = '/content/drive/My Drive/Projects Things Useful/University/CU Denver 2023-2024 Sem 1/Deep Learning/Quiz_5_Data'\n",
        "pickle_path = directory_path + \"/Pickle_Files\"\n",
        "# Check if the directory exists\n",
        "if os.path.exists(directory_path):\n",
        "    # List all files and directories in the specified path\n",
        "    files = os.listdir(directory_path)\n",
        "    print(\"Files and directories in '\", directory_path, \"' :\")\n",
        "    for i in files:\n",
        "      print(i)\n",
        "else:\n",
        "    print(\"The directory does not exist\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0720XWhCx-_U",
        "outputId": "33d24c22-3fb1-4273-d2df-13ffccf3e0c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Data Loaded\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "file_name= \"/training-set-100000.json\"\n",
        "training_path = f'{directory_path}/{file_name}'\n",
        "data = []\n",
        "with open(training_path, 'r') as file:\n",
        "    data = json.load(file)\n",
        "    if data:\n",
        "      print(\"Training Data Loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EP2bpHe4Ha7"
      },
      "source": [
        "## Step 1. Data Pre-Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZimqFVBH67-q",
        "outputId": "2ddaeac4-72b7-44b3-da4c-9586e782d033"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "# Download stopwords and wordnet data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Initialize stemmer and lemmatizer\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Load English stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def clean_text(text):\n",
        "    # Remove HTML tags using regex/Basic cleaning steps just to make sure that the data is clean\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "\n",
        "    # Remove special characters and digits\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "    # Lowercase all texts\n",
        "    text = text.lower()\n",
        "\n",
        "    # Tokenize text\n",
        "    words = text.split()\n",
        "\n",
        "    # Remove stopwords and apply stemming or lemmatization\n",
        "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
        "    # words = [stemmer.stem(word) for word in words if word not in stop_words]\n",
        "\n",
        "    # Join the words back into a single string\n",
        "    text = ' '.join(words)\n",
        "\n",
        "    return text\n",
        "\n",
        "# cleaned_texts = [clean_text(text) for text in texts]\n",
        "\n",
        "# # Now `cleaned_texts` contains the preprocessed article texts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dCFn5m8tzfKr",
        "outputId": "7182e1a7-bc70-4f70-b9b2-67f87b8c44ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing and saving completed.\n"
          ]
        }
      ],
      "source": [
        "# Assuming `data` is already loaded with the articles data in the above format\n",
        "# Combine title and abstract for each article and gather labels\n",
        "texts = [clean_text(d['title'] + ' ' + d['abstractText']) for d in data['articles']]\n",
        "labels = [d['meshMajor'] for d in data['articles']]\n",
        "\n",
        "# Initialize the tokenizer with a specific vocabulary size, and a filter for punctuation\n",
        "# Highest Value of Vocabulary Size I could do without running out of RAM\n",
        "vocabulary_size = 20000  # for example, you might choose the top 20,000 words\n",
        "tokenizer = Tokenizer(num_words=vocabulary_size, lower=True, oov_token='UNK', filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
        "\n",
        "tokenizer.fit_on_texts(texts)\n",
        "\n",
        "# Convert texts to sequences of integer indices\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "# Determine a suitable maximum sequence length based on the data distribution\n",
        "# Highest Value I could do without running out of RAM\n",
        "max_seq_length = int(np.percentile([len(seq) for seq in sequences], 90))\n",
        "\n",
        "# Pad the sequences so that they all have the same length\n",
        "data_padded = pad_sequences(sequences, maxlen=max_seq_length, padding='post')\n",
        "\n",
        "# Initialize MultiLabelBinarizer to convert the labels to a binary matrix\n",
        "mlb = MultiLabelBinarizer()\n",
        "\n",
        "# This absolutely devours RAM, use sparingly.\n",
        "# # Save the tokenizer, MultiLabelBinarizer, max_seq_length for later use\n",
        "# with open(pickle_path + '/preprocessing_objects.pickle', 'wb') as handle:\n",
        "#     pickle.dump({'tokenizer': tokenizer, 'mlb': mlb, 'max_seq_length': max_seq_length,\n",
        "#                  'data_padded': data_padded, 'label_data': label_data}, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "# Now `data_padded` contains the padded sequences and `label_data` contains the multi-hot encoded labels\n",
        "# `word_index` contains the word index, and `max_seq_length` is the length up to which sequences will be padded\n",
        "print(\"Preprocessing and saving completed.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kiw-XcFmbND0",
        "outputId": "b8ef1e97-d1c7-4c15-c892-1451ef4b4164"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean weight: 14.263883988209132\n",
            "Median weight: 5.836030036204353\n",
            "Standard Deviation: 18.370353726288904\n"
          ]
        }
      ],
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import numpy as np\n",
        "\n",
        "# Flatten all labels into a single list\n",
        "all_labels_weights = [label for sublist in labels for label in sublist]\n",
        "\n",
        "# Calculate class weights\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(all_labels_weights), y=all_labels_weights)\n",
        "\n",
        "# Create a dictionary to pass to model.fit\n",
        "class_weight_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
        "\n",
        "label_data = mlb.fit_transform(labels)\n",
        "\n",
        "weights = list(class_weight_dict.values())\n",
        "print(f\"Mean weight: {np.mean(weights)}\")\n",
        "print(f\"Median weight: {np.median(weights)}\")\n",
        "print(f\"Standard Deviation: {np.std(weights)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShjgDL7VervI"
      },
      "source": [
        "Observations: The mean weight is relatively high, which could be influenced by some extremely high weights for rare classes. The median weight is lower than the mean, indicating that the majority of your classes have lower weights, but there are enough high-weight classes to skew the mean upwards. A high standard deviation confirms this wide spread in class weights."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCgmv84AzBEw"
      },
      "source": [
        "## Prepping Data for ~~Roberta~~ BERT (Google Colab keeps crashing if I make any of my models too complex üò¢) Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200,
          "referenced_widgets": [
            "d922775429704eceb59b6a74068ce5ea",
            "b050672f61624c6e97c8b9c794cbc874",
            "ecec396ca6374de896661790dd255f22",
            "9d5fe4a74c9e4fa89b17fdd2ebc9bcc4",
            "7708a523cc824a52b980764a3443ca46",
            "6f0fcd0f11ec43d2aa5d484d9b1a7ccf",
            "993f3d17da8c45039af4867bb8802c98",
            "8433d906bd234e6c96b0d12873e4777c",
            "1377fea1088947c682991f0164d31581",
            "46bd8048bc6649f49c66936d54dffacd",
            "39b2149b893c4be58f950edce0da49f7",
            "7072f83659cc4ea696a830b0cd1596bf",
            "a4907733aae8427386092e6ff610b59f",
            "2333b1933cec42ab986f167827ed1d79",
            "e10820d098ea46518e687abf2da0f143",
            "9f8ba3a6b9094efcb63df4c01019f5ec",
            "02d9950b74e240d19deceb805e21128b",
            "6296ab8966454a3591bf6c219354802c",
            "4384a397276d48e5a06c5ca5ac671093",
            "c0a615e7a97c4aa98ad1ae68a2689af1",
            "841ff221eb054aafab73e49e03bb1c8d",
            "bc5bf10e7bd74a14b589cd3b62360f25",
            "35de7e821cbd4d5bac246d7d4b6d0c93",
            "eb6511f25c884f4db5db17dff115cd2b",
            "06e07c435e6b48169a6f5e5431f8aad4",
            "2b839d9d15da4142908ac1e4f44bff53",
            "b26281d289f040938f69463b4634db05",
            "ec27b2827b9149b0951c27d5a1b4a78a",
            "45ed7bc4fb2349c8bfffc54017ddb56f",
            "7e08efeb092c4d479ce917120a64588d",
            "fd7f4608bf6943c88f57f87eff79ed82",
            "03734c7109064781913b5e2d0cb0b44c",
            "d686beac371d4a46b49cfeb1f55889d9",
            "7dd2a5651c244ec2a50fcd6b891f7a9e",
            "d9b54b519c554329b39cad2f1284153d",
            "2c061ffe121c48e6a0b6407151f11f12",
            "9935e706d89b496ca75ab9e6026590b8",
            "d1a84998902f4a628c7135d98f5efe7b",
            "ef1f6486f725448abe0fd9801ed661c8",
            "f5a1db1248624dc19b5d0f7ee2211636",
            "b7974d2c85074b36a9e5e1b0007b6950",
            "011307bc9e62423487fe78b33fb7b85e",
            "d16c30f0d2104efe8c1fcd43e184e586",
            "4cd088698d864b01b563a3bf0ba83ca0"
          ]
        },
        "id": "i42pcjyBzGE2",
        "outputId": "7591312d-a1e2-426d-e2f4-806d58bde6f5"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d922775429704eceb59b6a74068ce5ea",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7072f83659cc4ea696a830b0cd1596bf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "35de7e821cbd4d5bac246d7d4b6d0c93",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7dd2a5651c244ec2a50fcd6b891f7a9e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import BertTokenizer\n",
        "import numpy as np\n",
        "\n",
        "tokenizer_bert = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "max_seq_length = int(np.percentile([len(seq) for seq in sequences], 90))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w7xUNZ1I8vk-",
        "outputId": "1df690bd-70ae-48c2-e422-b0c505dfd8c1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Tokenize the texts and create attention masks\n",
        "tokenizer_bert = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "max_seq_length = int(np.percentile([len(seq) for seq in sequences], 70))\n",
        "\n",
        "for text in texts:\n",
        "    encoded_dict = tokenizer_bert.encode_plus(\n",
        "                        text,\n",
        "                        add_special_tokens = True,  # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = max_seq_length,\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,\n",
        "                        return_tensors = 'np',  # Return numpy arrays\n",
        "                        truncation=True\n",
        "                   )\n",
        "\n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "input_ids = np.concatenate(input_ids, axis=0)\n",
        "attention_masks = np.concatenate(attention_masks, axis=0)\n",
        "\n",
        "# Data for the BERT model is different that it would be for a standard LSTM based approach\n",
        "X_Train_BERT = [input_ids, attention_masks]\n",
        "Y_Train_BERT = label_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TQIVhNyS0fb-",
        "outputId": "859990db-8747-4aa7-8a72-f4c1a148895f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded\n"
          ]
        }
      ],
      "source": [
        "# # This proved to be too memory intensive for Google Colab\n",
        "# # Load all preprocessing objects at once\n",
        "# with open(pickle_path + '/preprocessing_objects.pickle', 'rb') as handle:\n",
        "#     preprocessing_objects = pickle.load(handle)\n",
        "\n",
        "# tokenizer = preprocessing_objects['tokenizer']\n",
        "# mlb = preprocessing_objects['mlb']\n",
        "# max_seq_length = preprocessing_objects['max_seq_length']\n",
        "# # data_padded = preprocessing_objects['data_padded']\n",
        "# label_data = preprocessing_objects['label_data']\n",
        "# print(\"Loaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EFVdE5Z_0sj_",
        "outputId": "47ce1b69-2fe8-4ee1-9046-312ea5608040"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1. UNK: 1\n",
            "2. patient: 2\n",
            "3. cell: 3\n",
            "4. study: 4\n",
            "5. result: 5\n",
            "6. protein: 6\n",
            "7. p: 7\n",
            "8. group: 8\n",
            "9. effect: 9\n",
            "10. level: 10\n"
          ]
        }
      ],
      "source": [
        "n = 10\n",
        "for i, (word, index) in enumerate(tokenizer.word_index.items()):\n",
        "    print(f\"{i + 1}. {word}: {index}\")\n",
        "    if i >= n - 1:  # since index starts at 0, we use n - 1\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcbaalUyjYfo"
      },
      "source": [
        "## Step 2. Creating the Embedded Layer üê™"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NaRWhCcZ9ItM"
      },
      "source": [
        "### RUN THIS ONCE, RELOAD AND THEN AFTERWARDS JUST USE PICKLE FILE VERSION\n",
        "(IT TAKES FOREVER TO LOAD)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEl8T0_977FG",
        "outputId": "e480442b-03dd-4410-ee9f-1799fdc8f86c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding matrix saved.\n"
          ]
        }
      ],
      "source": [
        "from gensim.models import KeyedVectors\n",
        "\n",
        "embedding_path = directory_path + \"/BioWordVec_PubMed_MIMICIII_d200.vec.bin\"\n",
        "# Here we use BioWord2Vec rather than standard Word2Vec since its meant for more medical-centric text\n",
        "embeddings = KeyedVectors.load_word2vec_format(embedding_path, binary=True)\n",
        "\n",
        "\n",
        "\n",
        "# Get the word_index from the tokenizer\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "# Initialize the embedding matrix\n",
        "embedding_matrix = np.zeros((len(word_index) + 1, 200))  # Add 1 for padding token\n",
        "\n",
        "# Populate the embedding matrix\n",
        "for word, i in word_index.items():\n",
        "    # Check if the word is in the model\n",
        "    if word in embeddings.key_to_index:\n",
        "        # Get the embedding vector for the word\n",
        "        embedding_vector = embeddings[word]\n",
        "        # If an embedding was found, add it to the matrix\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "\n",
        "with open(pickle_path + \"/embedding_matrix_20000\", 'wb') as handle:\n",
        "    pickle.dump(embedding_matrix, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "print(\"Embedding matrix saved.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IGFwQ9ZOqsQ3"
      },
      "outputs": [],
      "source": [
        "# Create the embedding layer with the embedding matrix\n",
        "\n",
        "loaded_embedding_matrix = []\n",
        "with open(pickle_path + \"/embedding_matrix_20000\", 'rb') as handle:\n",
        "    loaded_embedding_matrix = pickle.load(handle)\n",
        "\n",
        "embedding_layer = Embedding(input_dim=len(word_index) + 1,\n",
        "                            output_dim=200,\n",
        "                            weights=[loaded_embedding_matrix],\n",
        "                            input_length=max_seq_length,  # As determined earlier\n",
        "                            trainable=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpYZx6fp8I4g"
      },
      "source": [
        "## Step 3. Create LSTM layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DkH5z7NCu01o",
        "outputId": "a1c405c1-236c-45ff-c9a9-9caaa56ce8e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 180, 200)          54306400  \n",
            "                                                                 \n",
            " bidirectional (Bidirection  (None, 180, 256)          336896    \n",
            " al)                                                             \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 180, 256)          0         \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirecti  (None, 128)               164352    \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               16512     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 22373)             2886117   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 57710277 (220.15 MB)\n",
            "Trainable params: 3403877 (12.98 MB)\n",
            "Non-trainable params: 54306400 (207.16 MB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Number of unique labels in MeSH classification\n",
        "num_labels = len(mlb.classes_)\n",
        "\n",
        "model = Sequential()\n",
        "# Add the pre-loaded embedding layer\n",
        "model.add(embedding_layer)\n",
        "# Add an LSTM layer\n",
        "model.add(Bidirectional(LSTM(units=128, return_sequences=True)))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Bidirectional(LSTM(units=64)))\n",
        "# model.add(Dropout(0.2))\n",
        "# Add a dense layer\n",
        "model.add(Dense(units=128, activation='relu'))\n",
        "# model.add(Dropout(0.2))\n",
        "# Output layer with a sigmoid activation for multi-label classification\n",
        "# Final layer for each label\n",
        "model.add(Dense(units=num_labels, activation='sigmoid'))\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[Precision(), Recall()])\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rWjoTUKrUoz"
      },
      "source": [
        "Lets try building a second model thats different because not gonna lie the other one was pretty bad lmfao at least with Recall() Kind of a bit of a skill issue tbh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nttjwaWBrMgW",
        "outputId": "dfa25f77-0c98-4d27-d272-48fb8b319202"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFBertModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_ids (InputLayer)         [(None, 148)]        0           []                               \n",
            "                                                                                                  \n",
            " attention_mask (InputLayer)    [(None, 148)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_ids[0][0]',              \n",
            "                                thPoolingAndCrossAt               'attention_mask[0][0]']         \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 148,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 128)          98432       ['tf_bert_model[0][1]']          \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 128)         512         ['dense[0][0]']                  \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 128)          0           ['batch_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 22373)        2886117     ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 112,467,301\n",
            "Trainable params: 2,984,805\n",
            "Non-trainable params: 109,482,496\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from transformers import TFBertModel, BertTokenizer\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.metrics import Precision, Recall\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l1_l2\n",
        "\n",
        "num_labels = len(mlb.classes_)\n",
        "\n",
        "# Load BERT model and tokenizer\n",
        "bert_model = 'bert-base-uncased'\n",
        "tokenizer = BertTokenizer.from_pretrained(bert_model)\n",
        "bert = TFBertModel.from_pretrained(bert_model)\n",
        "\n",
        "# Freeze BERT layers so Colab can actually run this, I imagine this would be alot better if I could mark this as trainable\n",
        "for layer in bert.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Model inputs\n",
        "input_ids = Input(shape=(max_seq_length,), dtype='int32', name='input_ids')\n",
        "attention_mask = Input(shape=(max_seq_length,), dtype='int32', name='attention_mask')\n",
        "\n",
        "# BERT embeddings\n",
        "embeddings = bert(input_ids, attention_mask=attention_mask)[1]\n",
        "\n",
        "# Additional layers\n",
        "x = Dense(128, activation='relu', kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4))(embeddings)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dropout(0.1)(x)\n",
        "output = Dense(num_labels, activation='sigmoid')(x)\n",
        "\n",
        "# Define the model\n",
        "bert_model = Model(inputs=[input_ids, attention_mask], outputs=output)\n",
        "\n",
        "# Compile the model with a learning rate scheduler\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "bert_model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=[Precision(), Recall()])\n",
        "\n",
        "bert_model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCTYO_I6CTxe"
      },
      "source": [
        "## Step 4. Train the model and pray that Google Colab Pro is Enough!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5pEIdRfNRcBi"
      },
      "outputs": [],
      "source": [
        "# Load the training data\n",
        "X_train = data_padded  # padded sequence data\n",
        "Y_train = label_data   # multi-hot encoded MeSH terms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TeLL-Zfs8PQv",
        "outputId": "8765d485-2321-4958-b3d4-71122089f62c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/35\n",
            "5625/5625 [==============================] - ETA: 0s - loss: 0.0054 - precision: 0.1051 - recall: 0.0519\n",
            "Epoch 1: saving model to /content/drive/My Drive/Projects Things Useful/University/CU Denver 2023-2024 Sem 1/Deep Learning/Quiz_5_Data/Pickle_Files/lstm_inter/model_epoch_more_ram_version01.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r5625/5625 [==============================] - 166s 27ms/step - loss: 0.0054 - precision: 0.1051 - recall: 0.0519 - val_loss: 0.0034 - val_precision: 0.6481 - val_recall: 0.1139\n",
            "Epoch 2/35\n",
            "5625/5625 [==============================] - ETA: 0s - loss: 0.0033 - precision: 0.6857 - recall: 0.1057\n",
            "Epoch 2: saving model to /content/drive/My Drive/Projects Things Useful/University/CU Denver 2023-2024 Sem 1/Deep Learning/Quiz_5_Data/Pickle_Files/lstm_inter/model_epoch_more_ram_version02.h5\n",
            "5625/5625 [==============================] - 147s 26ms/step - loss: 0.0033 - precision: 0.6857 - recall: 0.1057 - val_loss: 0.0031 - val_precision: 0.7389 - val_recall: 0.1211\n",
            "Epoch 3/35\n",
            "5624/5625 [============================>.] - ETA: 0s - loss: 0.0029 - precision: 0.7300 - recall: 0.1291\n",
            "Epoch 3: saving model to /content/drive/My Drive/Projects Things Useful/University/CU Denver 2023-2024 Sem 1/Deep Learning/Quiz_5_Data/Pickle_Files/lstm_inter/model_epoch_more_ram_version03.h5\n",
            "5625/5625 [==============================] - 145s 26ms/step - loss: 0.0029 - precision: 0.7301 - recall: 0.1291 - val_loss: 0.0028 - val_precision: 0.7616 - val_recall: 0.1354\n",
            "Epoch 4/35\n",
            "5623/5625 [============================>.] - ETA: 0s - loss: 0.0026 - precision: 0.7484 - recall: 0.1442\n",
            "Epoch 4: saving model to /content/drive/My Drive/Projects Things Useful/University/CU Denver 2023-2024 Sem 1/Deep Learning/Quiz_5_Data/Pickle_Files/lstm_inter/model_epoch_more_ram_version04.h5\n",
            "5625/5625 [==============================] - 142s 25ms/step - loss: 0.0026 - precision: 0.7484 - recall: 0.1442 - val_loss: 0.0026 - val_precision: 0.7303 - val_recall: 0.1615\n",
            "Epoch 5/35\n",
            "5623/5625 [============================>.] - ETA: 0s - loss: 0.0025 - precision: 0.7479 - recall: 0.1575\n",
            "Epoch 5: saving model to /content/drive/My Drive/Projects Things Useful/University/CU Denver 2023-2024 Sem 1/Deep Learning/Quiz_5_Data/Pickle_Files/lstm_inter/model_epoch_more_ram_version05.h5\n",
            "5625/5625 [==============================] - 135s 24ms/step - loss: 0.0025 - precision: 0.7478 - recall: 0.1575 - val_loss: 0.0024 - val_precision: 0.7331 - val_recall: 0.1689\n",
            "Epoch 6/35\n",
            "5623/5625 [============================>.] - ETA: 0s - loss: 0.0023 - precision: 0.7463 - recall: 0.1698\n",
            "Epoch 6: saving model to /content/drive/My Drive/Projects Things Useful/University/CU Denver 2023-2024 Sem 1/Deep Learning/Quiz_5_Data/Pickle_Files/lstm_inter/model_epoch_more_ram_version06.h5\n",
            "5625/5625 [==============================] - 135s 24ms/step - loss: 0.0023 - precision: 0.7463 - recall: 0.1698 - val_loss: 0.0024 - val_precision: 0.7531 - val_recall: 0.1761\n",
            "Epoch 7/35\n",
            "5623/5625 [============================>.] - ETA: 0s - loss: 0.0023 - precision: 0.7441 - recall: 0.1815\n",
            "Epoch 7: saving model to /content/drive/My Drive/Projects Things Useful/University/CU Denver 2023-2024 Sem 1/Deep Learning/Quiz_5_Data/Pickle_Files/lstm_inter/model_epoch_more_ram_version07.h5\n",
            "5625/5625 [==============================] - 135s 24ms/step - loss: 0.0023 - precision: 0.7441 - recall: 0.1815 - val_loss: 0.0023 - val_precision: 0.7507 - val_recall: 0.1807\n",
            "Epoch 8/35\n",
            "5625/5625 [==============================] - ETA: 0s - loss: 0.0022 - precision: 0.7411 - recall: 0.1917\n",
            "Epoch 8: saving model to /content/drive/My Drive/Projects Things Useful/University/CU Denver 2023-2024 Sem 1/Deep Learning/Quiz_5_Data/Pickle_Files/lstm_inter/model_epoch_more_ram_version08.h5\n",
            "5625/5625 [==============================] - 134s 24ms/step - loss: 0.0022 - precision: 0.7411 - recall: 0.1917 - val_loss: 0.0023 - val_precision: 0.7447 - val_recall: 0.1898\n",
            "Epoch 9/35\n",
            "5625/5625 [==============================] - ETA: 0s - loss: 0.0021 - precision: 0.7402 - recall: 0.2008\n",
            "Epoch 9: saving model to /content/drive/My Drive/Projects Things Useful/University/CU Denver 2023-2024 Sem 1/Deep Learning/Quiz_5_Data/Pickle_Files/lstm_inter/model_epoch_more_ram_version09.h5\n",
            "5625/5625 [==============================] - 134s 24ms/step - loss: 0.0021 - precision: 0.7402 - recall: 0.2008 - val_loss: 0.0022 - val_precision: 0.7366 - val_recall: 0.1997\n",
            "Epoch 10/35\n",
            "5625/5625 [==============================] - ETA: 0s - loss: 0.0021 - precision: 0.7398 - recall: 0.2086\n",
            "Epoch 10: saving model to /content/drive/My Drive/Projects Things Useful/University/CU Denver 2023-2024 Sem 1/Deep Learning/Quiz_5_Data/Pickle_Files/lstm_inter/model_epoch_more_ram_version10.h5\n",
            "5625/5625 [==============================] - 135s 24ms/step - loss: 0.0021 - precision: 0.7398 - recall: 0.2086 - val_loss: 0.0022 - val_precision: 0.7287 - val_recall: 0.2102\n",
            "Epoch 11/35\n",
            "5624/5625 [============================>.] - ETA: 0s - loss: 0.0020 - precision: 0.7387 - recall: 0.2160\n",
            "Epoch 11: saving model to /content/drive/My Drive/Projects Things Useful/University/CU Denver 2023-2024 Sem 1/Deep Learning/Quiz_5_Data/Pickle_Files/lstm_inter/model_epoch_more_ram_version11.h5\n",
            "5625/5625 [==============================] - 134s 24ms/step - loss: 0.0020 - precision: 0.7387 - recall: 0.2160 - val_loss: 0.0022 - val_precision: 0.7374 - val_recall: 0.2075\n",
            "Epoch 12/35\n",
            "5624/5625 [============================>.] - ETA: 0s - loss: 0.0020 - precision: 0.7381 - recall: 0.2221\n",
            "Epoch 12: saving model to /content/drive/My Drive/Projects Things Useful/University/CU Denver 2023-2024 Sem 1/Deep Learning/Quiz_5_Data/Pickle_Files/lstm_inter/model_epoch_more_ram_version12.h5\n",
            "5625/5625 [==============================] - 135s 24ms/step - loss: 0.0020 - precision: 0.7381 - recall: 0.2221 - val_loss: 0.0022 - val_precision: 0.7394 - val_recall: 0.2067\n",
            "Epoch 13/35\n",
            "5625/5625 [==============================] - ETA: 0s - loss: 0.0019 - precision: 0.7381 - recall: 0.2279\n",
            "Epoch 13: saving model to /content/drive/My Drive/Projects Things Useful/University/CU Denver 2023-2024 Sem 1/Deep Learning/Quiz_5_Data/Pickle_Files/lstm_inter/model_epoch_more_ram_version13.h5\n",
            "5625/5625 [==============================] - 135s 24ms/step - loss: 0.0019 - precision: 0.7381 - recall: 0.2279 - val_loss: 0.0022 - val_precision: 0.7165 - val_recall: 0.2175\n",
            "Epoch 14/35\n",
            "5625/5625 [==============================] - ETA: 0s - loss: 0.0019 - precision: 0.7378 - recall: 0.2329\n",
            "Epoch 14: saving model to /content/drive/My Drive/Projects Things Useful/University/CU Denver 2023-2024 Sem 1/Deep Learning/Quiz_5_Data/Pickle_Files/lstm_inter/model_epoch_more_ram_version14.h5\n",
            "5625/5625 [==============================] - 135s 24ms/step - loss: 0.0019 - precision: 0.7378 - recall: 0.2329 - val_loss: 0.0022 - val_precision: 0.7207 - val_recall: 0.2162\n",
            "Epoch 15/35\n",
            "5623/5625 [============================>.] - ETA: 0s - loss: 0.0019 - precision: 0.7384 - recall: 0.2383\n",
            "Epoch 15: saving model to /content/drive/My Drive/Projects Things Useful/University/CU Denver 2023-2024 Sem 1/Deep Learning/Quiz_5_Data/Pickle_Files/lstm_inter/model_epoch_more_ram_version15.h5\n",
            "5625/5625 [==============================] - 135s 24ms/step - loss: 0.0019 - precision: 0.7384 - recall: 0.2383 - val_loss: 0.0022 - val_precision: 0.7082 - val_recall: 0.2253\n",
            "Epoch 16/35\n",
            "5624/5625 [============================>.] - ETA: 0s - loss: 0.0019 - precision: 0.7379 - recall: 0.2431\n",
            "Epoch 16: saving model to /content/drive/My Drive/Projects Things Useful/University/CU Denver 2023-2024 Sem 1/Deep Learning/Quiz_5_Data/Pickle_Files/lstm_inter/model_epoch_more_ram_version16.h5\n",
            "5625/5625 [==============================] - 135s 24ms/step - loss: 0.0019 - precision: 0.7379 - recall: 0.2431 - val_loss: 0.0022 - val_precision: 0.7265 - val_recall: 0.2159\n",
            "Epoch 17/35\n",
            "5624/5625 [============================>.] - ETA: 0s - loss: 0.0018 - precision: 0.7383 - recall: 0.2478\n",
            "Epoch 17: saving model to /content/drive/My Drive/Projects Things Useful/University/CU Denver 2023-2024 Sem 1/Deep Learning/Quiz_5_Data/Pickle_Files/lstm_inter/model_epoch_more_ram_version17.h5\n",
            "5625/5625 [==============================] - 134s 24ms/step - loss: 0.0018 - precision: 0.7383 - recall: 0.2478 - val_loss: 0.0022 - val_precision: 0.6895 - val_recall: 0.2386\n",
            "Epoch 18/35\n",
            "5623/5625 [============================>.] - ETA: 0s - loss: 0.0018 - precision: 0.7398 - recall: 0.2519\n",
            "Epoch 18: saving model to /content/drive/My Drive/Projects Things Useful/University/CU Denver 2023-2024 Sem 1/Deep Learning/Quiz_5_Data/Pickle_Files/lstm_inter/model_epoch_more_ram_version18.h5\n",
            "5625/5625 [==============================] - 133s 24ms/step - loss: 0.0018 - precision: 0.7398 - recall: 0.2519 - val_loss: 0.0022 - val_precision: 0.6975 - val_recall: 0.2324\n",
            "Epoch 19/35\n",
            "5625/5625 [==============================] - ETA: 0s - loss: 0.0018 - precision: 0.7405 - recall: 0.2565\n",
            "Epoch 19: saving model to /content/drive/My Drive/Projects Things Useful/University/CU Denver 2023-2024 Sem 1/Deep Learning/Quiz_5_Data/Pickle_Files/lstm_inter/model_epoch_more_ram_version19.h5\n",
            "5625/5625 [==============================] - 133s 24ms/step - loss: 0.0018 - precision: 0.7405 - recall: 0.2565 - val_loss: 0.0022 - val_precision: 0.6977 - val_recall: 0.2263\n",
            "Epoch 20/35\n",
            "5623/5625 [============================>.] - ETA: 0s - loss: 0.0018 - precision: 0.7414 - recall: 0.2608\n",
            "Epoch 20: saving model to /content/drive/My Drive/Projects Things Useful/University/CU Denver 2023-2024 Sem 1/Deep Learning/Quiz_5_Data/Pickle_Files/lstm_inter/model_epoch_more_ram_version20.h5\n",
            "5625/5625 [==============================] - 135s 24ms/step - loss: 0.0018 - precision: 0.7414 - recall: 0.2608 - val_loss: 0.0022 - val_precision: 0.6848 - val_recall: 0.2404\n",
            "Epoch 20: early stopping\n",
            "Training completed.\n",
            "Model saved at /content/drive/My Drive/Projects Things Useful/University/CU Denver 2023-2024 Sem 1/Deep Learning/Quiz_5_Data/Pickle_Files/saved_model_35_Epochs_Recall_Class_Weight_Version_10.h5\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "# Set training parameters\n",
        "batch_size = 16\n",
        "epochs = 35\n",
        "validation_split = 0.1  # Percentage of data to use as validation\n",
        "\n",
        "# EarlyStopping callback\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1)\n",
        "\n",
        "# ModelCheckpoint callback\n",
        "model_checkpoint = ModelCheckpoint(\n",
        "    filepath=pickle_path + '/lstm_inter/model_epoch_more_ram_version{epoch:02d}.h5',\n",
        "    save_best_only=False,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "# class_weight=class_weight_dict removed after it actually HURT and I mean KILLED my RECALL like BRUH WHAT how did it do THAT badly with it.\n",
        "history = model.fit(X_train, Y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    validation_split=validation_split,\n",
        "                    callbacks=[early_stopping, model_checkpoint],  # Add ModelCheckpoint here\n",
        "                    verbose=1)\n",
        "\n",
        "print(\"Training completed.\")\n",
        "\n",
        "\n",
        "model_save_path = pickle_path + '/saved_model_35_Epochs_Recall_Class_Weight_Version_10.h5'  # Replace with your desired path\n",
        "model.save(model_save_path)\n",
        "print(f\"Model saved at {model_save_path}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZgGe_XTwWvU"
      },
      "source": [
        "## Time to train a second model to see which one is better, this one is transformer based (üòû running out of System RAM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G1nET5FKpz_a",
        "outputId": "2b556165-9075-4058-9fc3-235b06866825"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "2813/2813 [==============================] - 7268s 3s/step - loss: 0.0319 - precision_1: 0.0096 - recall_1: 0.0746 - val_loss: 0.0058 - val_precision_1: 0.0789 - val_recall_1: 0.0882\n",
            "Epoch 2/10\n",
            "2813/2813 [==============================] - 7135s 3s/step - loss: 0.0037 - precision_1: 0.6373 - recall_1: 0.0837 - val_loss: 0.0043 - val_precision_1: 0.2607 - val_recall_1: 0.0484\n",
            "Epoch 3/10\n",
            "2813/2813 [==============================] - 7237s 3s/step - loss: 0.0036 - precision_1: 0.6464 - recall_1: 0.0894 - val_loss: 0.0049 - val_precision_1: 0.3769 - val_recall_1: 0.1759\n",
            "Epoch 4/10\n",
            "2813/2813 [==============================] - 7285s 3s/step - loss: 0.0036 - precision_1: 0.6477 - recall_1: 0.0893 - val_loss: 0.0154 - val_precision_1: 0.0154 - val_recall_1: 0.0902\n",
            "Epoch 5/10\n",
            "2813/2813 [==============================] - 7382s 3s/step - loss: 0.0036 - precision_1: 0.6437 - recall_1: 0.0881 - val_loss: 0.0038 - val_precision_1: 0.4632 - val_recall_1: 0.1435\n",
            "Epoch 6/10\n",
            "2813/2813 [==============================] - 7322s 3s/step - loss: 0.0035 - precision_1: 0.6527 - recall_1: 0.0912 - val_loss: 0.0042 - val_precision_1: 0.5057 - val_recall_1: 0.1155\n",
            "Epoch 7/10\n",
            "2813/2813 [==============================] - 7362s 3s/step - loss: 0.0035 - precision_1: 0.6531 - recall_1: 0.0918 - val_loss: 0.0396 - val_precision_1: 0.0107 - val_recall_1: 0.2508\n",
            "Epoch 8/10\n",
            "2813/2813 [==============================] - 7370s 3s/step - loss: 0.0035 - precision_1: 0.6514 - recall_1: 0.0918 - val_loss: 0.0044 - val_precision_1: 0.4004 - val_recall_1: 0.1522\n",
            "Epoch 8: early stopping\n",
            "Training completed.\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "# Compile the second model\n",
        "bert_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[Precision(), Recall()])\n",
        "\n",
        "# Set training parameters\n",
        "batch_size = 32\n",
        "epochs = 10 # Averages about 1.5 hours per epoch any more than this will cause Google Colab to timeout my session ):\n",
        "validation_split = 0.1  # Use 10% of the training data for validation\n",
        "\n",
        "\n",
        "# Define the checkpoint path and filenames\n",
        "checkpoint_path = pickle_path + \"/bert-models/model-{epoch:04d}.ckpt\"  # Replace with your path and file naming scheme\n",
        "\n",
        "# Create a callback that saves the model's weights every epoch\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    filepath=checkpoint_path,\n",
        "    verbose=1,\n",
        "    save_weights_only=True,\n",
        "    save_freq='epoch'\n",
        ")\n",
        "\n",
        "\n",
        "# Define an EarlyStopping callback to prevent overfitting\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1)\n",
        "\n",
        "# Train the model\n",
        "history = bert_model.fit(X_Train_BERT, Y_Train_BERT,\n",
        "                           batch_size=batch_size,\n",
        "                           epochs=epochs,\n",
        "                           validation_split=validation_split,  # Use validation split\n",
        "                           callbacks=[early_stopping],\n",
        "                           verbose=1)\n",
        "\n",
        "print(\"Training completed.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXREx_tjSzIM"
      },
      "source": [
        "Oh the Colab Environemnt Crashed because we ran out of memory when training the BERT based model... (biggest constraint for this project is that the compute/computing environments I have access to are not them)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "buit2vLFRNuR"
      },
      "outputs": [],
      "source": [
        "model_save_path = pickle_path + '/saved_model_35_Epochs_Recall_Class_Weight_Version_10.h5'  # Replace with your desired path\n",
        "second_model_save_path = pickle_path + '/saved_model_25_Epochs_BERT.h5'  # Replace with your desired path\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HeyZgyX62MBj",
        "outputId": "1b6c1a3c-0473-4260-9075-bb52ca06daaf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model saved at /content/drive/My Drive/Projects Things Useful/University/CU Denver 2023-2024 Sem 1/Deep Learning/Quiz_5_Data/Pickle_Files/saved_model_25_Epochs_BERT.h5\n"
          ]
        }
      ],
      "source": [
        "# Save the trained model\n",
        "bert_model.save(second_model_save_path)\n",
        "print(f\"Model saved at {second_model_save_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ESbBHjojNeq"
      },
      "source": [
        "## Step 5. Call the saved model and run it. Give it a test run to see we're getting real results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X46pb-AbOPmz",
        "outputId": "11e75a69-66c7-401b-9917-fd64888cae4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully.\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load the model\n",
        "loaded_model = load_model(model_save_path)\n",
        "print(\"Model loaded successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2hg6Pfgadl0",
        "outputId": "4979290c-585b-4ca0-d3f0-60c4e9ff6eb4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded successfully.\n"
          ]
        }
      ],
      "source": [
        "from transformers import TFBertModel\n",
        "\n",
        "# Specify the custom object (TFBertModel in this case)\n",
        "custom_objects = {\"TFBertModel\": TFBertModel}\n",
        "\n",
        "# Load the model\n",
        "loaded_BERT_based_model = load_model(second_model_save_path, custom_objects=custom_objects)\n",
        "print(\"Model loaded successfully.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lak5hJYuheJz",
        "outputId": "7f461fe0-d059-4d98-85d6-50d7ceac8e6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 1s/step\n",
            "Predicted result: [[3.1992629e-06 3.3945948e-06 6.2403005e-06 ... 1.6966989e-04\n",
            "  2.1725267e-05 1.7836053e-05]]\n",
            "Actual result: [0 0 0 ... 0 0 0]\n"
          ]
        }
      ],
      "source": [
        "first_sample = X_train[1:2]  # Select the first sample\n",
        "predicted_result = loaded_model.predict(first_sample)  # Predict using the loaded model\n",
        "\n",
        "# Print the predicted result\n",
        "print(\"Predicted result:\", predicted_result)\n",
        "\n",
        "# Print the actual result\n",
        "actual_result = Y_train[1]\n",
        "print(\"Actual result:\", actual_result)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2zG-BuEohxnw",
        "outputId": "990a34ae-2e15-45db-b0fd-201114b9d7ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted MeSH terms: ('Female', 'Humans')\n",
            "Actual MeSH terms: ('Adult', 'Aged', 'Aged, 80 and over', 'Carcinoma, Squamous Cell', 'Case-Control Studies', 'Female', 'Gene Frequency', 'Genetic Predisposition to Disease', 'Genotype', 'Germany', 'Greece', 'Humans', 'Interleukin-8', 'Male', 'Middle Aged', 'Mouth Neoplasms', 'Polymorphism, Genetic', 'Reverse Transcriptase Polymerase Chain Reaction', 'Risk')\n"
          ]
        }
      ],
      "source": [
        "# Use inverse_transform to convert binary vectors back to labels\n",
        "import numpy as np\n",
        "\n",
        "# Convert predicted result to binary and then to labels\n",
        "predicted_labels_conformed = mlb.inverse_transform(np.round(predicted_result))\n",
        "\n",
        "# Ensure actual_result is a 2D array for inverse_transform\n",
        "actual_labels_conformed = mlb.inverse_transform(np.array([actual_result]))\n",
        "\n",
        "print(\"Predicted MeSH terms:\", predicted_labels_conformed[0])\n",
        "print(\"Actual MeSH terms:\", actual_labels_conformed[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ad_sRHa-Ofky"
      },
      "source": [
        "## Step 4. Seeing how well we did on the test set üôè"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5E9qYZtYqL7"
      },
      "source": [
        "NOTE: The LSTM model did better so if you have to grade one please grade that one - (though I am hoping that model performance isn't a large factor in this Quizzes grade üò≠)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dti4cX6xiRib",
        "outputId": "4d120e3e-fc6c-4015-8b4e-3e1929f26474"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Data Loaded\n"
          ]
        }
      ],
      "source": [
        "# Load the test set\n",
        "import json\n",
        "file_name= \"test-set-20000-rev2.json\"\n",
        "training_path = f'{directory_path}/{file_name}'\n",
        "data = []\n",
        "with open(training_path, 'r') as file:\n",
        "    data = json.load(file)\n",
        "    if data:\n",
        "      print(\"Test Data Loaded\")\n",
        "\n",
        "texts = [clean_text(d['title'] + ' ' + d['abstractText']) for d in data['documents']]\n",
        "labels = [d['meshMajor'] for d in data['documents']]\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "# Pad the sequences so that they all have the same length\n",
        "X_Test = data_padded_test = pad_sequences(sequences, maxlen=max_seq_length, padding='post')\n",
        "Y_Test = label_data_test = mlb.transform(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t2WeuvTSO9Fg",
        "outputId": "d78a1f21-a377-4eea-9dab-e03194816353"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "625/625 [==============================] - 45s 73ms/step\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import numpy as np\n",
        "\n",
        "# Make predictions on the test set\n",
        "test_predictions = loaded_model.predict(X_Test)\n",
        "binary_predictions = np.round(test_predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7yL9r_8nkbHj",
        "outputId": "c6b1b9a7-9823-4913-d36a-bad2c5e85b4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20000\n",
            "20000\n",
            "20000\n",
            "[0 0 0 ... 0 0 0]\n",
            "[2058 3561 3506 1692 3315   16    1 1301   11 1271  111 2058 3561 3506\n",
            " 1692 3315  155 1188  416  983   16    1 1301  103 2863  607   42 1301\n",
            "   44   62   11 3506 1247  121 2263   61    1  212 2058 3561 1692 3315\n",
            "  416   81 1301   33  761  416 2263  212 1353  643 2263  106   86  985\n",
            "  212   62 1188   81 1301  510 1365  111   20    5 1423 1379   10 1301\n",
            "   20  766 1081  943 1301    6   62  416 2263    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0]\n"
          ]
        }
      ],
      "source": [
        "print(len(Y_Test))\n",
        "print(len(X_Test))\n",
        "print(len(binary_predictions))\n",
        "\n",
        "print(Y_Test[0])\n",
        "print(X_Test[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFgwfA3bBBDD"
      },
      "source": [
        "## Metric Evaluations: LSTM Model with Embedding Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CAz108VHoKx2"
      },
      "outputs": [],
      "source": [
        "# Calculate the metrics\n",
        "accuracy = accuracy_score(Y_Test, binary_predictions)\n",
        "micro_precision = precision_score(Y_Test, binary_predictions, average='micro')\n",
        "micro_recall = recall_score(Y_Test, binary_predictions, average='micro')\n",
        "micro_f1 = f1_score(Y_Test, binary_predictions, average='micro')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QWIWKBXsgCxx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9acf5db6-4646-43bb-8586-628a221d9f07"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy on Test Data: 0.0\n",
            "Micro Precision on Test Data: 0.7820372398685652\n",
            "Micro Recall on Test Data: 0.10674241291672895\n",
            "Micro F1 Score on Test Data: 0.18784530386740333\n"
          ]
        }
      ],
      "source": [
        "print(\"Accuracy on Test Data:\", accuracy)\n",
        "print(\"Micro Precision on Test Data:\", micro_precision)\n",
        "print(\"Micro Recall on Test Data:\", micro_recall)\n",
        "print(\"Micro F1 Score on Test Data:\", micro_f1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYp-nSWoA2H1"
      },
      "source": [
        "## Metric Evaluations: BERT-Based Transformer Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I37P506JZU8T",
        "outputId": "2290fb09-8991-477f-c31e-4a9d6c34c4ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Data Loaded\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# BERT Transformer Based Mode:\n",
        "import json\n",
        "file_name= \"test-set-20000-rev2.json\"\n",
        "training_path = f'{directory_path}/{file_name}'\n",
        "data = []\n",
        "with open(training_path, 'r') as file:\n",
        "    data = json.load(file)\n",
        "    if data:\n",
        "      print(\"Test Data Loaded\")\n",
        "\n",
        "texts = [clean_text(d['title'] + ' ' + d['abstractText']) for d in data['documents']]\n",
        "labels = [d['meshMajor'] for d in data['documents']]\n",
        "\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "for text in texts:\n",
        "    encoded_dict = tokenizer_bert.encode_plus(\n",
        "                        text,\n",
        "                        add_special_tokens = True,  # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = max_seq_length,\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,\n",
        "                        return_tensors = 'np',  # Return numpy arrays\n",
        "                        truncation=True\n",
        "                   )\n",
        "\n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "input_ids = np.concatenate(input_ids, axis=0)\n",
        "attention_masks = np.concatenate(attention_masks, axis=0)\n",
        "\n",
        "# Data for the BERT model is different that it would be for a standard LSTM based approach\n",
        "X_Test_BERT = [input_ids, attention_masks]\n",
        "Y_Test_BERT = mlb.transform(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zf6V2I-rbSIN",
        "outputId": "d52feca5-63c2-42e9-ee35-83fc16651a2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "625/625 [==============================] - 1297s 2s/step\n"
          ]
        }
      ],
      "source": [
        "test_predictions_BERT = loaded_BERT_based_model.predict(X_Test_BERT)\n",
        "binary_predictions_BERT = np.round(test_predictions_BERT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kCs6Ek5hbdKa"
      },
      "outputs": [],
      "source": [
        "accuracy = accuracy_score(Y_Test_BERT, binary_predictions_BERT)\n",
        "micro_precision = precision_score(Y_Test_BERT, binary_predictions_BERT, average='micro')\n",
        "micro_recall = recall_score(Y_Test_BERT, binary_predictions_BERT, average='micro')\n",
        "micro_f1 = f1_score(Y_Test_BERT, binary_predictions_BERT, average='micro')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Muuc1uSdrgMI",
        "outputId": "defa78b8-7500-4051-ede2-03026ee392fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Transformer Model Performance\n",
            "Accuracy on Test Data: 0.0\n",
            "Micro Precision on Test Data: 0.39523151277885965\n",
            "Micro Recall on Test Data: 0.15015467414929218\n",
            "Micro F1 Score on Test Data: 0.2176287571531752\n"
          ]
        }
      ],
      "source": [
        "print(\"Transformer Model Performance\")\n",
        "print(\"Accuracy on Test Data:\", accuracy)\n",
        "print(\"Micro Precision on Test Data:\", micro_precision)\n",
        "print(\"Micro Recall on Test Data:\", micro_recall)\n",
        "print(\"Micro F1 Score on Test Data:\", micro_f1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0s0vNqtqCda"
      },
      "source": [
        "# Model Report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be6_G6M4qE6u"
      },
      "source": [
        "After evaluating my model on the test data for the multi-label classification task, the results suggest that it's moderately effective, but there's significant room for improvement. Notably, the model's accuracy is surprisingly low at 0.0, indicating it's not predicting all the labels for any single sample correctly. This is a known challenge in multi-label classification, where accuracy demands a perfect match of all labels.\n",
        "\n",
        "The micro precision, at 69.48%, shows that a majority of the predicted labels are correct. However, the micro recall is only 22.74%, implying that the model is missing many relevant labels. This is further reflected in the moderate micro F1 score of 34.27%, which balances precision and recall.\n",
        "\n",
        "The discrepancy between precision and recall suggests a conservative model that makes fewer errors in its predictions but overlooks a significant number of correct labels. This could stem from various factors, such as the model's complexity, potential class imbalance in the dataset, or the need for more extensive training. To enhance the model's effectiveness, fine-tuning, and a more in-depth analysis are crucial next steps.\n",
        "\n",
        "Furthermore, the transformer based model perhaps using the BERT model for may be worth it. However due to computational limitations (I had to freeze BERT's weights to even get the model to train on Google Colab) I was unable to dedicate any additional resources to further explore this beyond just 8 epochs of training. As it is now, its precision and recall was far inferior ot that of the LSTM model with an embedding layer.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4qP0uFPqeLW"
      },
      "source": [
        "# Step 6. Making Predictions for Judge Using the LSTM Based Model and Writing to JSON"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hGZ-YORasKkh",
        "outputId": "454f7096-8e1c-4a88-c19d-a2941121236c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Judge Data Loaded\n"
          ]
        }
      ],
      "source": [
        "# Load the test set\n",
        "import json\n",
        "file_name= \"judge-set-10000-unannotated.json\"\n",
        "training_path = f'{directory_path}/{file_name}'\n",
        "data = []\n",
        "with open(training_path, 'r') as file:\n",
        "    data = json.load(file)\n",
        "    if data:\n",
        "      print(\"Judge Data Loaded\")\n",
        "\n",
        "texts = [clean_text(d['title'] + ' ' + d['abstractText']) for d in data['documents']]\n",
        "labels = [d['pmid'] for d in data['documents']]\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "# Pad the sequences so that they all have the same length\n",
        "X_Judge = data_padded_test = pad_sequences(sequences, maxlen=max_seq_length, padding='post')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWphUweIslHJ",
        "outputId": "9ecf75bd-5ccb-42ac-cad5-9bfc604a68f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 22s 70ms/step\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import numpy as np\n",
        "\n",
        "# Make predictions on the judge set\n",
        "judge_predictions = loaded_model.predict(X_Judge)\n",
        "binary_predictions = np.round(judge_predictions)\n",
        "predicted_labels_conformed = mlb.inverse_transform((binary_predictions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YjJBXOI7s9TI",
        "outputId": "4b0f44a0-81fb-46d2-85c7-a2468da1245d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predictions saved to /content/drive/My Drive/Projects Things Useful/University/CU Denver 2023-2024 Sem 1/Deep Learning/Quiz_5_Data/predicted_labels.json\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "# Prepare data for JSON export\n",
        "json_output = {\"documents\": []}\n",
        "for pmid, labels in zip(labels, predicted_labels_conformed):\n",
        "    json_output[\"documents\"].append({\"pmid\": pmid, \"labels\": list(labels)})\n",
        "\n",
        "# Write the data to a JSON file\n",
        "output_path = f'{directory_path}/predicted_labels.json'\n",
        "with open(output_path, 'w') as file:\n",
        "    json.dump(json_output, file, indent=4)\n",
        "\n",
        "print(f\"Predictions saved to {output_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jWe__6xItinM"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyMNwlNNdAB9Aef8i9Er37Ov",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "011307bc9e62423487fe78b33fb7b85e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "02d9950b74e240d19deceb805e21128b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "03734c7109064781913b5e2d0cb0b44c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "06e07c435e6b48169a6f5e5431f8aad4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7e08efeb092c4d479ce917120a64588d",
            "max": 466062,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fd7f4608bf6943c88f57f87eff79ed82",
            "value": 466062
          }
        },
        "1377fea1088947c682991f0164d31581": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2333b1933cec42ab986f167827ed1d79": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4384a397276d48e5a06c5ca5ac671093",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c0a615e7a97c4aa98ad1ae68a2689af1",
            "value": 231508
          }
        },
        "2b839d9d15da4142908ac1e4f44bff53": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_03734c7109064781913b5e2d0cb0b44c",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_d686beac371d4a46b49cfeb1f55889d9",
            "value": " 466k/466k [00:00&lt;00:00, 7.45MB/s]"
          }
        },
        "2c061ffe121c48e6a0b6407151f11f12": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b7974d2c85074b36a9e5e1b0007b6950",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_011307bc9e62423487fe78b33fb7b85e",
            "value": 570
          }
        },
        "35de7e821cbd4d5bac246d7d4b6d0c93": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_eb6511f25c884f4db5db17dff115cd2b",
              "IPY_MODEL_06e07c435e6b48169a6f5e5431f8aad4",
              "IPY_MODEL_2b839d9d15da4142908ac1e4f44bff53"
            ],
            "layout": "IPY_MODEL_b26281d289f040938f69463b4634db05"
          }
        },
        "39b2149b893c4be58f950edce0da49f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4384a397276d48e5a06c5ca5ac671093": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "45ed7bc4fb2349c8bfffc54017ddb56f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "46bd8048bc6649f49c66936d54dffacd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4cd088698d864b01b563a3bf0ba83ca0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6296ab8966454a3591bf6c219354802c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6f0fcd0f11ec43d2aa5d484d9b1a7ccf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7072f83659cc4ea696a830b0cd1596bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a4907733aae8427386092e6ff610b59f",
              "IPY_MODEL_2333b1933cec42ab986f167827ed1d79",
              "IPY_MODEL_e10820d098ea46518e687abf2da0f143"
            ],
            "layout": "IPY_MODEL_9f8ba3a6b9094efcb63df4c01019f5ec"
          }
        },
        "7708a523cc824a52b980764a3443ca46": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7dd2a5651c244ec2a50fcd6b891f7a9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d9b54b519c554329b39cad2f1284153d",
              "IPY_MODEL_2c061ffe121c48e6a0b6407151f11f12",
              "IPY_MODEL_9935e706d89b496ca75ab9e6026590b8"
            ],
            "layout": "IPY_MODEL_d1a84998902f4a628c7135d98f5efe7b"
          }
        },
        "7e08efeb092c4d479ce917120a64588d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "841ff221eb054aafab73e49e03bb1c8d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8433d906bd234e6c96b0d12873e4777c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9935e706d89b496ca75ab9e6026590b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d16c30f0d2104efe8c1fcd43e184e586",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_4cd088698d864b01b563a3bf0ba83ca0",
            "value": " 570/570 [00:00&lt;00:00, 46.8kB/s]"
          }
        },
        "993f3d17da8c45039af4867bb8802c98": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9d5fe4a74c9e4fa89b17fdd2ebc9bcc4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_46bd8048bc6649f49c66936d54dffacd",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_39b2149b893c4be58f950edce0da49f7",
            "value": " 28.0/28.0 [00:00&lt;00:00, 1.95kB/s]"
          }
        },
        "9f8ba3a6b9094efcb63df4c01019f5ec": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a4907733aae8427386092e6ff610b59f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_02d9950b74e240d19deceb805e21128b",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_6296ab8966454a3591bf6c219354802c",
            "value": "vocab.txt: 100%"
          }
        },
        "b050672f61624c6e97c8b9c794cbc874": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6f0fcd0f11ec43d2aa5d484d9b1a7ccf",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_993f3d17da8c45039af4867bb8802c98",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "b26281d289f040938f69463b4634db05": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b7974d2c85074b36a9e5e1b0007b6950": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc5bf10e7bd74a14b589cd3b62360f25": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c0a615e7a97c4aa98ad1ae68a2689af1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d16c30f0d2104efe8c1fcd43e184e586": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d1a84998902f4a628c7135d98f5efe7b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d686beac371d4a46b49cfeb1f55889d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d922775429704eceb59b6a74068ce5ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b050672f61624c6e97c8b9c794cbc874",
              "IPY_MODEL_ecec396ca6374de896661790dd255f22",
              "IPY_MODEL_9d5fe4a74c9e4fa89b17fdd2ebc9bcc4"
            ],
            "layout": "IPY_MODEL_7708a523cc824a52b980764a3443ca46"
          }
        },
        "d9b54b519c554329b39cad2f1284153d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ef1f6486f725448abe0fd9801ed661c8",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_f5a1db1248624dc19b5d0f7ee2211636",
            "value": "config.json: 100%"
          }
        },
        "e10820d098ea46518e687abf2da0f143": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_841ff221eb054aafab73e49e03bb1c8d",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_bc5bf10e7bd74a14b589cd3b62360f25",
            "value": " 232k/232k [00:00&lt;00:00, 5.15MB/s]"
          }
        },
        "eb6511f25c884f4db5db17dff115cd2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ec27b2827b9149b0951c27d5a1b4a78a",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_45ed7bc4fb2349c8bfffc54017ddb56f",
            "value": "tokenizer.json: 100%"
          }
        },
        "ec27b2827b9149b0951c27d5a1b4a78a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ecec396ca6374de896661790dd255f22": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8433d906bd234e6c96b0d12873e4777c",
            "max": 28,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1377fea1088947c682991f0164d31581",
            "value": 28
          }
        },
        "ef1f6486f725448abe0fd9801ed661c8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f5a1db1248624dc19b5d0f7ee2211636": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fd7f4608bf6943c88f57f87eff79ed82": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}